{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYLZzIbjk62B"
   },
   "source": [
    "# Waymo Open Dataset Motion Tutorial\n",
    "\n",
    "- Website: https://waymo.com/open\n",
    "- GitHub: https://github.com/waymo-research/waymo-open-dataset\n",
    "\n",
    "This tutorial demonstrates:\n",
    "- How to decode and interpret the data.\n",
    "- How to train a simple model with Tensorflow.\n",
    "\n",
    "Visit the [Waymo Open Dataset Website](https://waymo.com/open) to download the full dataset.\n",
    "\n",
    "To use, open this notebook in [Colab](https://colab.research.google.com).\n",
    "\n",
    "Uncheck the box \"Reset all runtimes before running\" if you run this colab directly from the remote kernel. Alternatively, you can make a copy before trying to run it by following \"File > Save copy in Drive ...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ez4Nsk06Sqd"
   },
   "source": [
    "# Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WH4E-2DySUbT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: waymo-open-dataset-tf-2-12-0==1.6.4 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (1.6.4)\n",
      "Requirement already satisfied: absl-py==1.4.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.4.0)\n",
      "Requirement already satisfied: dask==2023.3.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (2023.3.1)\n",
      "Requirement already satisfied: einsum==0.3.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (0.3.0)\n",
      "Requirement already satisfied: google-auth==2.16.2 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (2.16.2)\n",
      "Requirement already satisfied: immutabledict==2.2.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (2.2.0)\n",
      "Requirement already satisfied: matplotlib==3.6.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (3.6.1)\n",
      "Requirement already satisfied: numpy==1.23 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.23.0)\n",
      "Requirement already satisfied: openexr==1.3.9 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.3.9)\n",
      "Requirement already satisfied: pandas==1.5.3 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.5.3)\n",
      "Requirement already satisfied: pillow==9.2.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (9.2.0)\n",
      "Requirement already satisfied: plotly==5.13.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (5.13.1)\n",
      "Requirement already satisfied: pyarrow==10.0.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (10.0.0)\n",
      "Requirement already satisfied: scikit-image==0.20.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (0.20.0)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.2.2)\n",
      "Requirement already satisfied: setuptools==67.6.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (67.6.0)\n",
      "Requirement already satisfied: tensorflow==2.12 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (2.12.0)\n",
      "Requirement already satisfied: tensorflow_graphics==2021.12.3 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (2021.12.3)\n",
      "Requirement already satisfied: tensorflow_probability==0.19.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (0.19.0)\n",
      "Requirement already satisfied: visu3d==1.5.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.5.1)\n",
      "Requirement already satisfied: dacite==1.8.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.8.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (3.1.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (24.2)\n",
      "Requirement already satisfied: partd>=1.2.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from google-auth==2.16.2->waymo-open-dataset-tf-2-12-0==1.6.4) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from google-auth==2.16.2->waymo-open-dataset-tf-2-12-0==1.6.4) (0.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from google-auth==2.16.2->waymo-open-dataset-tf-2-12-0==1.6.4) (1.17.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from google-auth==2.16.2->waymo-open-dataset-tf-2-12-0==1.6.4) (4.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from pandas==1.5.3->waymo-open-dataset-tf-2-12-0==1.6.4) (2024.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from plotly==5.13.1->waymo-open-dataset-tf-2-12-0==1.6.4) (9.0.0)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (1.13.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (3.4.2)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (2.36.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (2024.9.20)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (1.8.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (0.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from scikit-learn==1.2.2->waymo-open-dataset-tf-2-12-0==1.6.4) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from scikit-learn==1.2.2->waymo-open-dataset-tf-2-12-0==1.6.4) (3.5.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (1.68.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.12.1)\n",
      "Requirement already satisfied: jax>=0.3.15 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.4.30)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.20.3)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.37.1)\n",
      "Requirement already satisfied: tensorflow-addons>=0.10.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (0.23.0)\n",
      "Requirement already satisfied: tensorflow-datasets>=2.0.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (4.9.7)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (6.1.0)\n",
      "Requirement already satisfied: tqdm>=4.45.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (4.67.1)\n",
      "Requirement already satisfied: trimesh>=2.37.22 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (4.5.3)\n",
      "Requirement already satisfied: decorator in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow_probability==0.19.0->waymo-open-dataset-tf-2-12-0==1.6.4) (5.1.1)\n",
      "Requirement already satisfied: dm-tree in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow_probability==0.19.0->waymo-open-dataset-tf-2-12-0==1.6.4) (0.1.8)\n",
      "Requirement already satisfied: dataclass_array in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from visu3d==1.5.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.5.1)\n",
      "Requirement already satisfied: einops in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from visu3d==1.5.1->waymo-open-dataset-tf-2-12-0==1.6.4) (0.8.0)\n",
      "Requirement already satisfied: etils[edc,enp,epath,epy,etree] in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from visu3d==1.5.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.11.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.44.0)\n",
      "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.4.30)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.5.0)\n",
      "Requirement already satisfied: locket in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from partd>=1.2.0->dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth==2.16.2->waymo-open-dataset-tf-2-12-0==1.6.4) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.1.3)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow-addons>=0.10.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (2.13.3)\n",
      "Requirement already satisfied: promise in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (2.3)\n",
      "Requirement already satisfied: simple-parsing in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (0.1.6)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (1.16.1)\n",
      "Requirement already satisfied: toml in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (0.10.2)\n",
      "Requirement already satisfied: array-record>=0.5.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (0.5.1)\n",
      "Requirement already satisfied: importlib_resources in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]->visu3d==1.5.1->waymo-open-dataset-tf-2-12-0==1.6.4) (6.4.5)\n",
      "Requirement already satisfied: zipp in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]->visu3d==1.5.1->waymo-open-dataset-tf-2-12-0==1.6.4) (3.21.0)\n",
      "Requirement already satisfied: lark in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from dataclass_array->visu3d==1.5.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.0.2)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from simple-parsing->tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (0.16)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/haibin/miniconda3/envs/Visualization/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install waymo-open-dataset-tf-2-12-0==1.6.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjT3Rdd4lSqC"
   },
   "source": [
    "# Imports and global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xdEcN6WilcBn"
   },
   "outputs": [],
   "source": [
    "# Data location. Please edit.\n",
    "\n",
    "# A tfrecord containing tf.Example protos as downloaded from the Waymo dataset\n",
    "# webpage.\n",
    "\n",
    "# Replace this path with your own tfrecords.\n",
    "FILENAME = '/data/haibin/ML_DM/render/training_20s.tfrecord-00000-of-01000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "M5gzSlBTlTiS"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from waymo_open_dataset.metrics.ops import py_metrics_ops\n",
    "from waymo_open_dataset.metrics.python import config_util_py as config_util\n",
    "from waymo_open_dataset.protos import motion_metrics_pb2\n",
    "\n",
    "# If you use a custom conversion from Scenario to tf.Example, set the correct\n",
    "# number of map samples here.\n",
    "num_map_samples = 30000\n",
    "\n",
    "# Example field definition\n",
    "roadgraph_features = {\n",
    "    'roadgraph_samples/dir': tf.io.FixedLenFeature(\n",
    "        [num_map_samples, 3], tf.float32, default_value=None\n",
    "    ),\n",
    "    'roadgraph_samples/id': tf.io.FixedLenFeature(\n",
    "        [num_map_samples, 1], tf.int64, default_value=None\n",
    "    ),\n",
    "    'roadgraph_samples/type': tf.io.FixedLenFeature(\n",
    "        [num_map_samples, 1], tf.int64, default_value=None\n",
    "    ),\n",
    "    'roadgraph_samples/valid': tf.io.FixedLenFeature(\n",
    "        [num_map_samples, 1], tf.int64, default_value=None\n",
    "    ),\n",
    "    'roadgraph_samples/xyz': tf.io.FixedLenFeature(\n",
    "        [num_map_samples, 3], tf.float32, default_value=None\n",
    "    ),\n",
    "}\n",
    "# Features of other agents.\n",
    "state_features = {\n",
    "    'state/id':\n",
    "        tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
    "    'state/type':\n",
    "        tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
    "    'state/is_sdc':\n",
    "        tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
    "    'state/tracks_to_predict':\n",
    "        tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
    "    'state/current/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/height':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/length':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.int64, default_value=None),\n",
    "    'state/current/valid':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.int64, default_value=None),\n",
    "    'state/current/vel_yaw':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/velocity_x':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/width':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/x':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/y':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/z':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/future/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/height':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/length':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.int64, default_value=None),\n",
    "    'state/future/valid':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.int64, default_value=None),\n",
    "    'state/future/vel_yaw':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/velocity_x':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/width':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/x':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/y':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/z':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/past/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/height':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/length':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.int64, default_value=None),\n",
    "    'state/past/valid':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.int64, default_value=None),\n",
    "    'state/past/vel_yaw':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/velocity_x':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/width':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/x':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/y':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/z':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "}\n",
    "\n",
    "traffic_light_features = {\n",
    "    'traffic_light_state/current/state':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/current/valid':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/current/x':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/current/y':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/current/z':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/past/state':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/past/valid':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/past/x':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/past/y':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/past/z':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "}\n",
    "\n",
    "features_description = {}\n",
    "features_description.update(roadgraph_features)\n",
    "features_description.update(state_features)\n",
    "features_description.update(traffic_light_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trAv9YGrvYnc"
   },
   "source": [
    "# Visualize TF Example sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWnysu4X7Wkt"
   },
   "source": [
    "## Create Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TpEZq1EMtXV9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 01:52:47.825296: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-12-06 01:52:47.843026: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: roadgraph_samples/dir (data type: float) is required but could not be found.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ParseExampleV2_Tdense_55_num_sparse_0_ragged_split_types_0_ragged_value_types_0_sparse_types_0_device_/job:localhost/replica:0/task:0/device:CPU:0}} Feature: roadgraph_samples/dir (data type: float) is required but could not be found. [Op:ParseExampleV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTFRecordDataset(FILENAME, compression_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mas_numpy_iterator())\n\u001b[0;32m----> 3\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_single_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_description\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Visualization/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/Visualization/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ParseExampleV2_Tdense_55_num_sparse_0_ragged_split_types_0_ragged_value_types_0_sparse_types_0_device_/job:localhost/replica:0/task:0/device:CPU:0}} Feature: roadgraph_samples/dir (data type: float) is required but could not be found. [Op:ParseExampleV2]"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
    "data = next(dataset.as_numpy_iterator())\n",
    "parsed = tf.io.parse_single_example(data, features_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdc8CBg27dtn"
   },
   "source": [
    "## Generate visualization images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utTE9Mtgx3Fq"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 196\u001b[0m\n\u001b[1;32m    191\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(im)\n\u001b[1;32m    193\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m images\n\u001b[0;32m--> 196\u001b[0m images \u001b[38;5;241m=\u001b[39m visualize_all_agents_smooth(\u001b[43mparsed\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed' is not defined"
     ]
    }
   ],
   "source": [
    "def create_figure_and_axes(size_pixels):\n",
    "  \"\"\"Initializes a unique figure and axes for plotting.\"\"\"\n",
    "  fig, ax = plt.subplots(1, 1, num=uuid.uuid4())\n",
    "\n",
    "  # Sets output image to pixel resolution.\n",
    "  dpi = 100\n",
    "  size_inches = size_pixels / dpi\n",
    "  fig.set_size_inches([size_inches, size_inches])\n",
    "  fig.set_dpi(dpi)\n",
    "  fig.set_facecolor('white')\n",
    "  ax.set_facecolor('white')\n",
    "  ax.xaxis.label.set_color('black')\n",
    "  ax.tick_params(axis='x', colors='black')\n",
    "  ax.yaxis.label.set_color('black')\n",
    "  ax.tick_params(axis='y', colors='black')\n",
    "  fig.set_tight_layout(True)\n",
    "  ax.grid(False)\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "def fig_canvas_image(fig):\n",
    "  \"\"\"Returns a [H, W, 3] uint8 np.array image from fig.canvas.tostring_rgb().\"\"\"\n",
    "  # Just enough margin in the figure to display xticks and yticks.\n",
    "  fig.subplots_adjust(\n",
    "      left=0.08, bottom=0.08, right=0.98, top=0.98, wspace=0.0, hspace=0.0)\n",
    "  fig.canvas.draw()\n",
    "  data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  return data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "\n",
    "def get_colormap(num_agents):\n",
    "  \"\"\"Compute a color map array of shape [num_agents, 4].\"\"\"\n",
    "  colors = cm.get_cmap('jet', num_agents)\n",
    "  colors = colors(range(num_agents))\n",
    "  np.random.shuffle(colors)\n",
    "  return colors\n",
    "\n",
    "\n",
    "def get_viewport(all_states, all_states_mask):\n",
    "  \"\"\"Gets the region containing the data.\n",
    "\n",
    "  Args:\n",
    "    all_states: states of agents as an array of shape [num_agents, num_steps,\n",
    "      2].\n",
    "    all_states_mask: binary mask of shape [num_agents, num_steps] for\n",
    "      `all_states`.\n",
    "\n",
    "  Returns:\n",
    "    center_y: float. y coordinate for center of data.\n",
    "    center_x: float. x coordinate for center of data.\n",
    "    width: float. Width of data.\n",
    "  \"\"\"\n",
    "  valid_states = all_states[all_states_mask]\n",
    "  all_y = valid_states[..., 1]\n",
    "  all_x = valid_states[..., 0]\n",
    "\n",
    "  center_y = (np.max(all_y) + np.min(all_y)) / 2\n",
    "  center_x = (np.max(all_x) + np.min(all_x)) / 2\n",
    "\n",
    "  range_y = np.ptp(all_y)\n",
    "  range_x = np.ptp(all_x)\n",
    "\n",
    "  width = max(range_y, range_x)\n",
    "\n",
    "  return center_y, center_x, width\n",
    "\n",
    "\n",
    "def visualize_one_step(states,\n",
    "                       mask,\n",
    "                       roadgraph,\n",
    "                       title,\n",
    "                       center_y,\n",
    "                       center_x,\n",
    "                       width,\n",
    "                       color_map,\n",
    "                       size_pixels=1000):\n",
    "  \"\"\"Generate visualization for a single step.\"\"\"\n",
    "\n",
    "  # Create figure and axes.\n",
    "  fig, ax = create_figure_and_axes(size_pixels=size_pixels)\n",
    "\n",
    "  # Plot roadgraph.\n",
    "  rg_pts = roadgraph[:, :2].T\n",
    "  ax.plot(rg_pts[0, :], rg_pts[1, :], 'k.', alpha=1, ms=2)\n",
    "\n",
    "  masked_x = states[:, 0][mask]\n",
    "  masked_y = states[:, 1][mask]\n",
    "  colors = color_map[mask]\n",
    "\n",
    "  # Plot agent current position.\n",
    "  ax.scatter(\n",
    "      masked_x,\n",
    "      masked_y,\n",
    "      marker='o',\n",
    "      linewidths=3,\n",
    "      color=colors,\n",
    "  )\n",
    "\n",
    "  # Title.\n",
    "  ax.set_title(title)\n",
    "\n",
    "  # Set axes.  Should be at least 10m on a side and cover 160% of agents.\n",
    "  size = max(10, width * 1.0)\n",
    "  ax.axis([\n",
    "      -size / 2 + center_x, size / 2 + center_x, -size / 2 + center_y,\n",
    "      size / 2 + center_y\n",
    "  ])\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "  image = fig_canvas_image(fig)\n",
    "  plt.close(fig)\n",
    "  return image\n",
    "\n",
    "\n",
    "def visualize_all_agents_smooth(\n",
    "    decoded_example,\n",
    "    size_pixels=1000,\n",
    "):\n",
    "  \"\"\"Visualizes all agent predicted trajectories in a serie of images.\n",
    "\n",
    "  Args:\n",
    "    decoded_example: Dictionary containing agent info about all modeled agents.\n",
    "    size_pixels: The size in pixels of the output image.\n",
    "\n",
    "  Returns:\n",
    "    T of [H, W, 3] uint8 np.arrays of the drawn matplotlib's figure canvas.\n",
    "  \"\"\"\n",
    "  # [num_agents, num_past_steps, 2] float32.\n",
    "  past_states = tf.stack(\n",
    "      [decoded_example['state/past/x'], decoded_example['state/past/y']],\n",
    "      -1).numpy()\n",
    "  past_states_mask = decoded_example['state/past/valid'].numpy() > 0.0\n",
    "\n",
    "  # [num_agents, 1, 2] float32.\n",
    "  current_states = tf.stack(\n",
    "      [decoded_example['state/current/x'], decoded_example['state/current/y']],\n",
    "      -1).numpy()\n",
    "  current_states_mask = decoded_example['state/current/valid'].numpy() > 0.0\n",
    "\n",
    "  # [num_agents, num_future_steps, 2] float32.\n",
    "  future_states = tf.stack(\n",
    "      [decoded_example['state/future/x'], decoded_example['state/future/y']],\n",
    "      -1).numpy()\n",
    "  future_states_mask = decoded_example['state/future/valid'].numpy() > 0.0\n",
    "\n",
    "  # [num_points, 3] float32.\n",
    "  roadgraph_xyz = decoded_example['roadgraph_samples/xyz'].numpy()\n",
    "\n",
    "  num_agents, num_past_steps, _ = past_states.shape\n",
    "  num_future_steps = future_states.shape[1]\n",
    "\n",
    "  color_map = get_colormap(num_agents)\n",
    "\n",
    "  # [num_agens, num_past_steps + 1 + num_future_steps, depth] float32.\n",
    "  all_states = np.concatenate([past_states, current_states, future_states], 1)\n",
    "\n",
    "  # [num_agens, num_past_steps + 1 + num_future_steps] float32.\n",
    "  all_states_mask = np.concatenate(\n",
    "      [past_states_mask, current_states_mask, future_states_mask], 1)\n",
    "\n",
    "  center_y, center_x, width = get_viewport(all_states, all_states_mask)\n",
    "\n",
    "  images = []\n",
    "\n",
    "  # Generate images from past time steps.\n",
    "  for i, (s, m) in enumerate(\n",
    "      zip(\n",
    "          np.split(past_states, num_past_steps, 1),\n",
    "          np.split(past_states_mask, num_past_steps, 1))):\n",
    "    im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
    "                            'past: %d' % (num_past_steps - i), center_y,\n",
    "                            center_x, width, color_map, size_pixels)\n",
    "    images.append(im)\n",
    "\n",
    "  # Generate one image for the current time step.\n",
    "  s = current_states\n",
    "  m = current_states_mask\n",
    "\n",
    "  im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz, 'current', center_y,\n",
    "                          center_x, width, color_map, size_pixels)\n",
    "  images.append(im)\n",
    "\n",
    "  # Generate images from future time steps.\n",
    "  for i, (s, m) in enumerate(\n",
    "      zip(\n",
    "          np.split(future_states, num_future_steps, 1),\n",
    "          np.split(future_states_mask, num_future_steps, 1))):\n",
    "    im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
    "                            'future: %d' % (i + 1), center_y, center_x, width,\n",
    "                            color_map, size_pixels)\n",
    "    images.append(im)\n",
    "\n",
    "  return images\n",
    "\n",
    "\n",
    "images = visualize_all_agents_smooth(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrIZjUHG7hM3"
   },
   "source": [
    "## Display animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tt2IeGiG0eny"
   },
   "outputs": [],
   "source": [
    "def create_animation(images):\n",
    "  \"\"\" Creates a Matplotlib animation of the given images.\n",
    "\n",
    "  Args:\n",
    "    images: A list of numpy arrays representing the images.\n",
    "\n",
    "  Returns:\n",
    "    A matplotlib.animation.Animation.\n",
    "\n",
    "  Usage:\n",
    "    anim = create_animation(images)\n",
    "    anim.save('/tmp/animation.avi')\n",
    "    HTML(anim.to_html5_video())\n",
    "  \"\"\"\n",
    "\n",
    "  plt.ioff()\n",
    "  fig, ax = plt.subplots()\n",
    "  dpi = 100\n",
    "  size_inches = 1000 / dpi\n",
    "  fig.set_size_inches([size_inches, size_inches])\n",
    "  plt.ion()\n",
    "\n",
    "  def animate_func(i):\n",
    "    ax.imshow(images[i])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid('off')\n",
    "\n",
    "  anim = animation.FuncAnimation(\n",
    "      fig, animate_func, frames=len(images) // 2, interval=100)\n",
    "  plt.close(fig)\n",
    "  return anim\n",
    "\n",
    "\n",
    "anim = create_animation(images[::5])\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdOQTZAiuKdQ"
   },
   "source": [
    "# Simple MLP model with TF\n",
    "\n",
    "Note that this is a very simple example model to demonstrate inputs parsing and metrics computation. Not at all competitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "b_5G9lx9uK9B"
   },
   "outputs": [],
   "source": [
    "def _parse(value):\n",
    "  decoded_example = tf.io.parse_single_example(value, features_description)\n",
    "\n",
    "  past_states = tf.stack([\n",
    "      decoded_example['state/past/x'], decoded_example['state/past/y'],\n",
    "      decoded_example['state/past/length'], decoded_example['state/past/width'],\n",
    "      decoded_example['state/past/bbox_yaw'],\n",
    "      decoded_example['state/past/velocity_x'],\n",
    "      decoded_example['state/past/velocity_y']\n",
    "  ], -1)\n",
    "\n",
    "  cur_states = tf.stack([\n",
    "      decoded_example['state/current/x'], decoded_example['state/current/y'],\n",
    "      decoded_example['state/current/length'],\n",
    "      decoded_example['state/current/width'],\n",
    "      decoded_example['state/current/bbox_yaw'],\n",
    "      decoded_example['state/current/velocity_x'],\n",
    "      decoded_example['state/current/velocity_y']\n",
    "  ], -1)\n",
    "\n",
    "  input_states = tf.concat([past_states, cur_states], 1)[..., :2]\n",
    "\n",
    "  future_states = tf.stack([\n",
    "      decoded_example['state/future/x'], decoded_example['state/future/y'],\n",
    "      decoded_example['state/future/length'],\n",
    "      decoded_example['state/future/width'],\n",
    "      decoded_example['state/future/bbox_yaw'],\n",
    "      decoded_example['state/future/velocity_x'],\n",
    "      decoded_example['state/future/velocity_y']\n",
    "  ], -1)\n",
    "\n",
    "  gt_future_states = tf.concat([past_states, cur_states, future_states], 1)\n",
    "\n",
    "  past_is_valid = decoded_example['state/past/valid'] > 0\n",
    "  current_is_valid = decoded_example['state/current/valid'] > 0\n",
    "  future_is_valid = decoded_example['state/future/valid'] > 0\n",
    "  gt_future_is_valid = tf.concat(\n",
    "      [past_is_valid, current_is_valid, future_is_valid], 1)\n",
    "\n",
    "  # If a sample was not seen at all in the past, we declare the sample as\n",
    "  # invalid.\n",
    "  sample_is_valid = tf.reduce_any(\n",
    "      tf.concat([past_is_valid, current_is_valid], 1), 1)\n",
    "\n",
    "  inputs = {\n",
    "      'input_states': input_states,\n",
    "      'gt_future_states': gt_future_states,\n",
    "      'gt_future_is_valid': gt_future_is_valid,\n",
    "      'object_type': decoded_example['state/type'],\n",
    "      'tracks_to_predict': decoded_example['state/tracks_to_predict'] > 0,\n",
    "      'sample_is_valid': sample_is_valid,\n",
    "  }\n",
    "  return inputs\n",
    "\n",
    "\n",
    "def _default_metrics_config():\n",
    "  config = motion_metrics_pb2.MotionMetricsConfig()\n",
    "  config_text = \"\"\"\n",
    "  track_steps_per_second: 10\n",
    "  prediction_steps_per_second: 2\n",
    "  track_history_samples: 10\n",
    "  track_future_samples: 80\n",
    "  speed_lower_bound: 1.4\n",
    "  speed_upper_bound: 11.0\n",
    "  speed_scale_lower: 0.5\n",
    "  speed_scale_upper: 1.0\n",
    "  step_configurations {\n",
    "    measurement_step: 5\n",
    "    lateral_miss_threshold: 1.0\n",
    "    longitudinal_miss_threshold: 2.0\n",
    "  }\n",
    "  step_configurations {\n",
    "    measurement_step: 9\n",
    "    lateral_miss_threshold: 1.8\n",
    "    longitudinal_miss_threshold: 3.6\n",
    "  }\n",
    "  step_configurations {\n",
    "    measurement_step: 15\n",
    "    lateral_miss_threshold: 3.0\n",
    "    longitudinal_miss_threshold: 6.0\n",
    "  }\n",
    "  max_predictions: 6\n",
    "  \"\"\"\n",
    "  text_format.Parse(config_text, config)\n",
    "  return config\n",
    "\n",
    "\n",
    "class SimpleModel(tf.keras.Model):\n",
    "  \"\"\"A simple one-layer regressor.\"\"\"\n",
    "\n",
    "  def __init__(self, num_agents_per_scenario, num_states_steps,\n",
    "               num_future_steps):\n",
    "    super(SimpleModel, self).__init__()\n",
    "    self._num_agents_per_scenario = num_agents_per_scenario\n",
    "    self._num_states_steps = num_states_steps\n",
    "    self._num_future_steps = num_future_steps\n",
    "    self.regressor = tf.keras.layers.Dense(num_future_steps * 2)\n",
    "\n",
    "  def call(self, states):\n",
    "    states = tf.reshape(states, (-1, self._num_states_steps * 2))\n",
    "    pred = self.regressor(states)\n",
    "    pred = tf.reshape(\n",
    "        pred, [-1, self._num_agents_per_scenario, self._num_future_steps, 2])\n",
    "    return pred\n",
    "\n",
    "\n",
    "class MotionMetrics(tf.keras.metrics.Metric):\n",
    "  \"\"\"Wrapper for motion metrics computation.\"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self._prediction_trajectory = []\n",
    "    self._prediction_score = []\n",
    "    self._ground_truth_trajectory = []\n",
    "    self._ground_truth_is_valid = []\n",
    "    self._prediction_ground_truth_indices = []\n",
    "    self._prediction_ground_truth_indices_mask = []\n",
    "    self._object_type = []\n",
    "    self._metrics_config = config\n",
    "\n",
    "  def reset_state():\n",
    "    self._prediction_trajectory = []\n",
    "    self._prediction_score = []\n",
    "    self._ground_truth_trajectory = []\n",
    "    self._ground_truth_is_valid = []\n",
    "    self._prediction_ground_truth_indices = []\n",
    "    self._prediction_ground_truth_indices_mask = []\n",
    "    self._object_type = []\n",
    "\n",
    "  def update_state(self, prediction_trajectory, prediction_score,\n",
    "                   ground_truth_trajectory, ground_truth_is_valid,\n",
    "                   prediction_ground_truth_indices,\n",
    "                   prediction_ground_truth_indices_mask, object_type):\n",
    "    self._prediction_trajectory.append(prediction_trajectory)\n",
    "    self._prediction_score.append(prediction_score)\n",
    "    self._ground_truth_trajectory.append(ground_truth_trajectory)\n",
    "    self._ground_truth_is_valid.append(ground_truth_is_valid)\n",
    "    self._prediction_ground_truth_indices.append(\n",
    "        prediction_ground_truth_indices)\n",
    "    self._prediction_ground_truth_indices_mask.append(\n",
    "        prediction_ground_truth_indices_mask)\n",
    "    self._object_type.append(object_type)\n",
    "\n",
    "  def result(self):\n",
    "    # [batch_size, num_preds, 1, 1, steps, 2].\n",
    "    # The ones indicate top_k = 1, num_agents_per_joint_prediction = 1.\n",
    "    prediction_trajectory = tf.concat(self._prediction_trajectory, 0)\n",
    "    # [batch_size, num_preds, 1].\n",
    "    prediction_score = tf.concat(self._prediction_score, 0)\n",
    "    # [batch_size, num_agents, gt_steps, 7].\n",
    "    ground_truth_trajectory = tf.concat(self._ground_truth_trajectory, 0)\n",
    "    # [batch_size, num_agents, gt_steps].\n",
    "    ground_truth_is_valid = tf.concat(self._ground_truth_is_valid, 0)\n",
    "    # [batch_size, num_preds, 1].\n",
    "    prediction_ground_truth_indices = tf.concat(\n",
    "        self._prediction_ground_truth_indices, 0)\n",
    "    # [batch_size, num_preds, 1].\n",
    "    prediction_ground_truth_indices_mask = tf.concat(\n",
    "        self._prediction_ground_truth_indices_mask, 0)\n",
    "    # [batch_size, num_agents].\n",
    "    object_type = tf.cast(tf.concat(self._object_type, 0), tf.int64)\n",
    "\n",
    "    # We are predicting more steps than needed by the eval code. Subsample.\n",
    "    interval = (\n",
    "        self._metrics_config.track_steps_per_second //\n",
    "        self._metrics_config.prediction_steps_per_second)\n",
    "    prediction_trajectory = prediction_trajectory[...,\n",
    "                                                  (interval - 1)::interval, :]\n",
    "\n",
    "    return py_metrics_ops.motion_metrics(\n",
    "        config=self._metrics_config.SerializeToString(),\n",
    "        prediction_trajectory=prediction_trajectory,\n",
    "        prediction_score=prediction_score,\n",
    "        ground_truth_trajectory=ground_truth_trajectory,\n",
    "        ground_truth_is_valid=ground_truth_is_valid,\n",
    "        prediction_ground_truth_indices=prediction_ground_truth_indices,\n",
    "        prediction_ground_truth_indices_mask=prediction_ground_truth_indices_mask,\n",
    "        object_type=object_type)\n",
    "\n",
    "\n",
    "model = SimpleModel(128, 11, 80)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "metrics_config = _default_metrics_config()\n",
    "motion_metrics = MotionMetrics(metrics_config)\n",
    "metric_names = config_util.get_breakdown_names_from_motion_config(\n",
    "    metrics_config)\n",
    "\n",
    "\n",
    "def train_step(inputs):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # [batch_size, num_agents, D]\n",
    "    states = inputs['input_states']\n",
    "\n",
    "    # Predict. [batch_size, num_agents, steps, 2].\n",
    "    pred_trajectory = model(states, training=True)\n",
    "\n",
    "    # Set training target.\n",
    "    prediction_start = metrics_config.track_history_samples + 1\n",
    "\n",
    "    # [batch_size, num_agents, steps, 7]\n",
    "    gt_trajectory = inputs['gt_future_states']\n",
    "    gt_targets = gt_trajectory[..., prediction_start:, :2]\n",
    "\n",
    "    # [batch_size, num_agents, steps]\n",
    "    gt_is_valid = inputs['gt_future_is_valid']\n",
    "    # [batch_size, num_agents, steps]\n",
    "    weights = (\n",
    "        tf.cast(inputs['gt_future_is_valid'][..., prediction_start:],\n",
    "                tf.float32) *\n",
    "        tf.cast(inputs['tracks_to_predict'][..., tf.newaxis], tf.float32))\n",
    "\n",
    "    loss_value = loss_fn(gt_targets, pred_trajectory, sample_weight=weights)\n",
    "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "  # [batch_size, num_agents, steps, 2] ->\n",
    "  # [batch_size, num_agents, 1, 1, steps, 2].\n",
    "  # The added dimensions are top_k = 1, num_agents_per_joint_prediction = 1.\n",
    "  pred_trajectory = pred_trajectory[:, :, tf.newaxis, tf.newaxis]\n",
    "\n",
    "  # Fake the score since this model does not generate any score per predicted\n",
    "  # trajectory.\n",
    "  pred_score = tf.ones(shape=tf.shape(pred_trajectory)[:3])\n",
    "\n",
    "  # [batch_size, num_agents].\n",
    "  object_type = inputs['object_type']\n",
    "\n",
    "  # [batch_size, num_agents].\n",
    "  batch_size = tf.shape(inputs['tracks_to_predict'])[0]\n",
    "  num_samples = tf.shape(inputs['tracks_to_predict'])[1]\n",
    "\n",
    "  pred_gt_indices = tf.range(num_samples, dtype=tf.int64)\n",
    "  # [batch_size, num_agents, 1].\n",
    "  pred_gt_indices = tf.tile(pred_gt_indices[tf.newaxis, :, tf.newaxis],\n",
    "                            (batch_size, 1, 1))\n",
    "  # [batch_size, num_agents, 1].\n",
    "  pred_gt_indices_mask = inputs['tracks_to_predict'][..., tf.newaxis]\n",
    "\n",
    "  motion_metrics.update_state(pred_trajectory, pred_score, gt_trajectory,\n",
    "                              gt_is_valid, pred_gt_indices,\n",
    "                              pred_gt_indices_mask, object_type)\n",
    "\n",
    "  return loss_value\n",
    "\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(FILENAME)\n",
    "dataset = dataset.map(_parse)\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "epochs = 2\n",
    "num_batches_per_epoch = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print('\\nStart of epoch %d' % (epoch,))\n",
    "  start_time = time.time()\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, batch in enumerate(dataset):\n",
    "    loss_value = train_step(batch)\n",
    "\n",
    "    # Log every 10 batches.\n",
    "    if step % 10 == 0:\n",
    "      print('Training loss (for one batch) at step %d: %.4f' %\n",
    "            (step, float(loss_value)))\n",
    "      print('Seen so far: %d samples' % ((step + 1) * 64))\n",
    "\n",
    "    if step >= num_batches_per_epoch:\n",
    "      break\n",
    "\n",
    "  # Display metrics at the end of each epoch.\n",
    "  train_metric_values = motion_metrics.result()\n",
    "  for i, m in enumerate(\n",
    "      ['min_ade', 'min_fde', 'miss_rate', 'overlap_rate', 'map']):\n",
    "    for j, n in enumerate(metric_names):\n",
    "      print('{}/{}: {}'.format(m, n, train_metric_values[i, j]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Waymo Open Dataset Motion Tutorial",
   "private_outputs": true,
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Visualization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
